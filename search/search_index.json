{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hi-there-this-page-aims-to-be-an-eternal-work-in-progress","title":"Hi there! This page aims to be an eternal work in progress \ud83d\udea7","text":"Tip <p>New labs with Helm and Gitlab Operator, click here!</p> <p> Cheatsheets jq, awk, oc, git</p> <p> Labs Deployments, experiments</p> <p> Monitoring PromQL expressions</p> <p> ARO  Azure RedHat OpenShift</p> <ul> <li> <p>ABOUT ME</p> <p></p> <p>Site Reliability Engineer at Red Hat Inc. with 7y of hands-on experience with  K8S/OpenShift.</p> <p>I hold an MBA in Data Analysis, affirming my passion for data-driven decision-making. </p> <p>This has led me to enjoy exploring the Machine Learning world with <code>import sklearn</code>.</p> <p>During my journey, I had the opportunity to work closely with customers to configure observability stacks, such as Prometheus, ElasticSearch, Loki, and Grafana. Adivising on sizing recommendations, configurations and troubleshooting problems.</p> <p>I am diving now with  while working on Azure cloud and it's ecosystem.</p> <p> let's connect</p> </li> </ul> <p>Disclaimer</p> <p>These notes are my own comprehension and lessons learned. Does not represent Red Hat, nor replaces official documentation.</p>"},{"location":"blog/","title":"Index","text":"<p>Disclaimer</p> <p>These notes are my own comprehension and lessons learned. Does not represent Red Hat, nor replaces official documentation.</p> <ul> <li> <p> Cheatsheets</p> <p>Oneliner commands using jq, grep, awk, git.</p> <p> Here</p> </li> <li> <p> Monitoring</p> <p>PromQL expressions.</p> <p> Here</p> </li> <li> <p> Labs</p> <p>All sort of experiments.</p> <p> Here</p> </li> <li> <p> ARO</p> <p>Azure RedHat OpenShift</p> <p> Here</p> </li> </ul>"},{"location":"blog/aro/az-aro-commands/","title":"The AZ show","text":"<p>Some helpful <code>az</code> commands to inspect the cluster information.</p>","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#resource-groups","title":"Resource Groups","text":"<p>There are two resource group for ARO:</p> <ul> <li> <p>CLUSTER Resource Group: Can be any name, and it is created by the user.</p> </li> <li> <p>MANAGED Resource Group: The name has the following convention <code>aro-xxx</code>, and hosts resources like:</p> <ul> <li>The storageaccount, disks, load balancers, virtual machines.</li> <li>It is protected with <code>Deny Assignment</code>, to ensure that won't be tampered by the user.</li> </ul> </li> </ul>","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#how-to-find-out-the-aro-managed-resource-group","title":"How to find out the ARO 'managed' resource group.","text":"","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#method-1","title":"Method 1","text":"<pre><code>MANAGED_RG=\"aro-$(az aro show -n $CLUSTERNAME -g $CLUSTER_RESOURCEGROUP --query 'clusterProfile.domain' -o tsv)\"; echo $MANAGED_RG\n</code></pre>","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#method-2","title":"Method 2","text":"<ol> <li> <p>Get the ARO cluster <code>RESOURCEID</code>, exporting as variable: <pre><code>RESOURCEID=$(az aro show -n $CLUSTERNAME -g $CLUSTER_RESOURCEGROUP --query 'id' -o tsv) ; echo $RESOURCEID\n</code></pre></p> </li> <li> <p>Get the Managed Resource Group, exporting in a variable <pre><code>MANAGED_RG=$(az group list --query \"[?managedBy=='$RESOURCEID'].name\" -o tsv) ; echo $MANAGED_RG\n</code></pre></p> </li> </ol>","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#method-3","title":"Method 3","text":"<p>Alternatively, the full JSON response: <pre><code>az group list --query \"[?managedBy=='$RESOURCEID']\"\n</code></pre></p>","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#storage-accounts","title":"Storage Accounts","text":"","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#storage-lockdown","title":"Storage Lockdown","text":"<p>Check if Storage Lockdown is enabled for Storage Account Cluster and Image Registry (this is set by default). <code>AllowBlobPublicAccess</code> must be set to false. This is a default Azure feature.</p> <pre><code>az storage account list -g $MANAGED_RG --query \"[].{NAME:name, AllowBlobPublicAccess:allowBlobPublicAccess,MinimumTlsVersion:minimumTlsVersion}\" -o table\n</code></pre>","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#networking","title":"Networking","text":"","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#public-or-private","title":"Public or Private?","text":"<ul> <li> <p>API and Ingress <pre><code>az aro show -n $CLUSTERNAME -g $CLUSTER_RESOURCEGROUP --query='{api:apiserverProfile.visibility,ingress:ingressProfiles[*].{name: name,visibility: visibility}}'\n</code></pre></p> </li> <li> <p>Did I set UDR (UserDefinedRouting)? <pre><code>az aro show -n $CLUSTERNAME -g $CLUSTER_RESOURCEGROUP --query 'networkProfile.outboundType'\n</code></pre></p> </li> </ul>","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/aro/az-aro-commands/#service-principal","title":"Service Principal","text":"<ul> <li>What is the ServicePrincipal attached to my cluster? <pre><code>az aro show -n $CLUSTERNAME -g CLUSTER_RESOURCEGROUP --query 'servicePrincipalProfile.clientId' -o tsv\n</code></pre></li> </ul>","tags":["az, aro, cli, cheatsheet"]},{"location":"blog/cheatsheet/git-rebase-amend/","title":"Git rebase and amend","text":"<p>When handling dozens of different branches in a Git repo, doing a rebase and all the right commands to make a clean push can be a bit challenging. Here are my notes on how rebase and push it with more confidence.</p>","tags":["git, branches, rebase, amend"]},{"location":"blog/cheatsheet/git-rebase-amend/#git-real-life-operations","title":"Git real life operations","text":"","tags":["git, branches, rebase, amend"]},{"location":"blog/cheatsheet/git-rebase-amend/#scenario","title":"Scenario","text":"<ul> <li>Main as default branch</li> <li>Branch named <code>feature1</code> and <code>feature2</code> , both created from the same head of <code>main</code>.</li> <li>Branch <code>feature2</code> is already merged and I am still working on <code>feature-1</code>.</li> <li>Branch <code>feature1</code> already has a saved commit. Eg: <code>1a2bc3d4</code></li> </ul>","tags":["git, branches, rebase, amend"]},{"location":"blog/cheatsheet/git-rebase-amend/#purpose","title":"Purpose","text":"<p>The purpose here, is to add new changes to an existing commit from my branch <code>feature1</code>. The outcome of it, would be to not have 2 commits, in the same PR (Pull Request). </p>","tags":["git, branches, rebase, amend"]},{"location":"blog/cheatsheet/git-rebase-amend/#what-to-do","title":"What to do?","text":"<ol> <li> <p>Pull latest changes to my local <code>main</code> branch.  <pre><code>git checkout main\ngit pull\n</code></pre></p> </li> <li> <p>Rebase the new branch (<code>feature2</code>) from <code>main</code>. Assuming that the latest changes does not affect my own changes, rebasing now from <code>main</code> should Succeed. <pre><code>git checkout feature2\ngit rebase main\n</code></pre></p> </li> </ol> Note <p>Running <code>git rebase</code> modifies the commit SHA. Meaning, that SHA <code>1a2bc3d4</code> will change to another hash.</p> <ol> <li>Do the additional necessary changes. Once done, stage it them: <pre><code>git add .\n</code></pre></li> </ol> Note <p>Running <code>git add .</code> adds all new and modified files to stage. It differs from <code>git add -u</code> and <code>git add -A</code>. To check what changes were added to stage. Run <code>git status</code>.</p> <ol> <li> <p>Let's now ADD these new changes to our current existing commit. The flag <code>--no-edit</code> means that I will not change my original commit message. <pre><code>git commit --amend --no-edit\n</code></pre></p> </li> <li> <p>Push the changes of my local branch (origin) to my remote branch. The flag <code>-f</code> is because I am changing the history of commits. <pre><code>git push origin feature2 -f\n</code></pre></p> </li> </ol> <p>Disclaimer</p> <p>Running <code>git push origin</code> without adding the branch as parameter, will add all commits to the push.</p> <p>Voila! Once this is done, I should be able to see the new commit SHA, with my new changes in my PR that is already open.</p>","tags":["git, branches, rebase, amend"]},{"location":"blog/cheatsheet/git-stash/","title":"The safe world of Git stash","text":"<p>On this exercise, we will go through the \"hidden\" and safe world of using <code>git stash</code>. </p>","tags":["git, stash, commit"]},{"location":"blog/cheatsheet/git-stash/#context","title":"Context","text":"<p>As we start with git, we are often hammered with commands like, git commit/git diff/git push/git pull. But when it comes to real life usage in a big repository and hundreds branches, there is a whole more that kicks in. Eg. <code>git rebase</code> and <code>git amend</code>, as we covered in a previous post, click HERE to go to it.</p> <p>But what if Im still working in my changes and not ready to commit them, but still need to be saved to make tests or to resume work after something ???</p> <p>Introducing ... <code>git stash</code>.</p>","tags":["git, stash, commit"]},{"location":"blog/cheatsheet/git-stash/#git-stash","title":"git stash","text":"<p>This is meant to save your local changes temporarely. For every \"stashed\" saved, a new index will be created that can be applied or dropped.</p> command Description git stash push -u -m \"message\" Stash untracked files and current changes git stash list List the stash index git stash pop The stash index is applied AND removed from the list git stash apply The stash index is applied AND kept in the list git stash drop The stash index is discarded git stash show -p stash@{index-number} shows detailed diff <p>I definetely learned this in the hard way, but it is practical and makes things safer while work is still in progress. \ud83d\ude01</p>","tags":["git, stash, commit"]},{"location":"blog/labs/gitlab-operator-in-tosa/","title":"Setup Gitlab Operator in ROSA","text":"<p>On this exercise, we will deploy the Operator and setup the Gitlab instance. </p> <p>This setup took me a while to figure out the missing pieces, but it was fun to go through and make it minimally working (It is using self-signed certificates).</p>","tags":["gitlab, operators, aws, openshift"]},{"location":"blog/labs/gitlab-operator-in-tosa/#environment","title":"Environment","text":"<pre><code>Red Hat OpenShift in AWS version: 4.14.25\nCommunity cert-manager operator version: 1.14.2 \nGitlab Operator version 1.0.0\nGitlab chart version: 8.0.0\n</code></pre>","tags":["gitlab, operators, aws, openshift"]},{"location":"blog/labs/gitlab-operator-in-tosa/#install-the-operator-and-setup-the-cr","title":"Install the Operator and setup the CR","text":"<ol> <li>Create the Ingress Class for Nginx. <pre><code>apiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n</code></pre></li> <li>Install the Gitlab Operator Certified version from Operator Hub.</li> <li>The custom resource (CR) for Gitlab. <pre><code>apiVersion: apps.gitlab.com/v1beta1\nkind: GitLab\nmetadata:\n  name: gitlab\n  namespace: gitlab-system\nspec:\n  chart:\n    values:\n      certmanager:\n        install: false\n      global:\n        hosts:\n          domain: apps.$BASEDOMAIN\n          hostSuffix: null\n        ingress:\n          class: gitlab-nginx\n          configureCertmanager: false\n          tls:\n            secretName: wildcard-gitlab-tls\n    version: 8.0.0\n</code></pre></li> </ol> Note <p>Once the Gitlab instance is created, all deployments/Ingress/services will be deployed, but the Ingress Controllers will be missing. To fix that, apply the RBAC and SCC required and provided by Gitlab repository. <code>oc apply -f https://gitlab.com/api/v4/projects/18899486/packages/generic/gitlab-operator/1.0.0/gitlab-operator-kubernetes-1.0.0.yaml</code></p> <p>Wait some minutes so the operator can reconcile.</p> <ol> <li> <p>In AWS  Route 53, in both Private and Public Hosted Zones, create the record:</p> <ul> <li>Record name: gitlab.apps.$BASEDOMAIN</li> <li>Value: The $EXTERNAL_IP from the service <code>gitlab-nginx-ingress-controller</code></li> <li>Type: CNAME</li> </ul> </li> <li> <p>Voila! \ud83d\ude80 Our self-served Gitlab instance is up and running! Login to the UI. Username <code>root</code>, and the password can be fetched in the secret <code>gitlab-gitlab-initial-root-password</code>: <pre><code>oc get secret/gitlab-gitlab-initial-root-password -n gitlab-system --template='{{index .data \"password\" | base64decode}}'\n</code></pre></p> </li> </ol>","tags":["gitlab, operators, aws, openshift"]},{"location":"blog/cheatsheet/grep-awk-filters/","title":"Parsing messages with grep and awk","text":"<p>When handlind hundreds of log files for inspection, knowing how to use grep to parse and find the relevant errors saves a lot of time. I usually use grep for that, and in this note, I've saved some awk notes.</p>","tags":["grep, awk"]},{"location":"blog/cheatsheet/grep-awk-filters/#grep","title":"GREP","text":"<ul> <li> <p>Returns all the directories that \"message\" appears. <pre><code>egrep \"message\" -rc * 2&gt;/dev/null | grep -v :0\n</code></pre></p> </li> <li> <p>Grep by time and \"message\". <pre><code>cat journalblah.txt | grep 'Sep 20 23:' | grep 'message' | tail -n 1\ncat journalblah.txt | grep 'Sep 20' | grep 'message' | wc -l\n</code></pre></p> </li> </ul>","tags":["grep, awk"]},{"location":"blog/cheatsheet/grep-awk-filters/#awk","title":"AWK","text":"<pre><code>'{}' = Action Block\nNR = Number of Records (built-in variable) ---&gt; Used to specify the line number of a set of text\n</code></pre>","tags":["grep, awk"]},{"location":"blog/cheatsheet/grep-awk-filters/#examples","title":"Examples","text":"<ul> <li>Assign variable to awk capture of word <pre><code>adjetivo=`awk -v number=$number 'NR==number{ print $1 }' $file_adjetivos`\n</code></pre></li> <li>FOR loop to interact with 2 columns of values as variables <pre><code>count=$(wc -l list-audit.log)\nfor i in {1..$count}; do node=`awk -v i=$i 'NR==i { print $1}' list-audit.log` ; file=`awk -v i...audit.log`; oc adm node-logs $node --path=oauth-apiserver\\/$file &gt; $node_$file.txt ; done\n</code></pre></li> <li>Arrays + awk <pre><code>declare -a arr=() ; for i in $(oc get nodes --no-headers | awk '{print $1}'); do arr+=( \"$i\"); echo $i ; done\ndeclare -a arr=() ; for i in $(oc get nodes --no-headers | awk '{print $1}'); do arr+=( \"$i\"); oc get nodes $i -o custom-columns=NAME:.metadata.name ; done\n</code></pre></li> <li>Filtering by field <pre><code>$ awk -F \"|\" '$5 &lt; 4000 ' file.txt\nOR\n$ awk -F \"|\" -v low_salary=\"4000\" '$5 &lt; low_salary ' file.txt\n$ awk -F \"|\" -v low_salary=\"4000\" '$5 &lt; low_salary { print $4 } ' file.txt\nOR\nawk -F \"|\" -v low_salary=\"4000\" -v high_salary=4500 -v header=\"------my header-------\" 'BEGIN { print header } $5 &gt;= high_salary || $5 &lt;= low_salary { print $2, $5}' pipe_example.txt\n\n08|garca|branca|branca_a_garca@gmail.com|1000\n09|micael|o gato|micael@gmail.com|2000\n\n/// file.txt\n06|amazonia|mosquiteira|am_mosquiteira@gmail.com|4500\n07|jacare|pantanoso|pantanoso@gmail.com|4000\n08|garca|branca|branca_a_garca@gmail.com|1000\n09|micael|o gato|micael@gmail.com|2000\n</code></pre></li> </ul>","tags":["grep, awk"]},{"location":"blog/labs/helmchart-from-scratch/","title":"Helmchart from scratch using OpenShift local","text":"<p>On this exercise, we will deploy an application from scratch using Helm Chart.</p>","tags":["crc, helmchart, openshift"]},{"location":"blog/labs/helmchart-from-scratch/#environment-pre-requisites","title":"Environment &amp; Pre-requisites","text":"<ul> <li>Helm Chart docs</li> </ul>","tags":["crc, helmchart, openshift"]},{"location":"blog/labs/helmchart-from-scratch/#used-binaries","title":"Used binaries","text":"<pre><code>CRC version: 2.36.0+27c493\nOpenShift version: 4.15.12\nPodman version: 4.4.4\nhelm version:\nversion.BuildInfo{Version:\"v3.13.2+35.el9\", GitCommit:\"fa6e939d7984e1be0d6fbc2dc920b6bbcf395932\", GitTreeState:\"clean\", GoVersion:\"go1.20.12\"}\n</code></pre>","tags":["crc, helmchart, openshift"]},{"location":"blog/labs/helmchart-from-scratch/#setup-registry-authentication","title":"Setup registry authentication","text":"<p>If using OpenShift, it is necessary to update the pullsecret in the cluster with the necessary registry authentication. To verify: <pre><code>\u276f oc get secret/pull-secret -n openshift-config --template='{{index .data \".dockerconfigjson\" | base64decode}}'\n</code></pre></p>","tags":["crc, helmchart, openshift"]},{"location":"blog/labs/helmchart-from-scratch/#starting-with-helm","title":"Starting with helm","text":"<ol> <li> <p>With <code>helm create</code> generate the directory with the required structure to start using Helm. <pre><code>\u276f helm create mynginx\n\u276f tree mynginx\nmynginx\n\u251c\u2500\u2500 charts -&gt; empty by default. Used for adding dependent charts\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 templates -&gt; Configuration files that deploys in the cluster\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 deployment.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _helpers.tpl\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hpa.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ingress.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 NOTES.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 serviceaccount.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 service.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 values.yaml\n\n4 directories, 10 files\n</code></pre></p> </li> <li> <p>Modify the <code>values.yaml</code> to contain the deployment definitions like image, serviceaccount, service port. <pre><code>repository: nginx\n\nservice:\n  type: ClusterIP\n  port: 80\n\nserviceAccount:\n  name: \"sa-nginx\"\n</code></pre></p> </li> <li> <p>Deploy the application with <code>helm install</code>. <pre><code>\u276f helm install mynginx-chart mynginx/ --values mynginx/values.yaml\nNAME: mynginx-chart\nLAST DEPLOYED: Sat May 25 12:56:37 2024\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=mynginx,app.kubernetes.io/instance=mynginx-chart\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT\n</code></pre></p> </li> <li> <p>Verify that the pod is running      <pre><code>\u276f oc get pods\nNAME                                       READY   STATUS    RESTARTS   AGE\nmynginx-chart-mynginx-664dfbb4b4-r7xqq   1/1     Running   0          13s\n</code></pre></p> </li> <li> <p>Expose the application service, and <code>curl</code> the route to verify if is up and running.     <pre><code>\u276f oc expose svc mynginx-chart-mynginx\nroute.route.openshift.io/mynginx-chart-mynginx exposed\n\u276f oc get route\nNAME                      HOST/PORT                                          PATH   SERVICES                  PORT   TERMINATION   WILDCARD\nmynginx-chart-mynginx   mynginx-chart-mynginx-default.apps-crc.testing          mynginx-chart-mynginx   http                 None\n\n\u276f curl -s -o /dev/null -w \"remote_ip: %{remote_ip}\\nremote_port: %{remote_port}\\nresponse_code: %{response_code}\\n\" http://$ROUTE_NAME\nremote_ip: 192.168.130.11\nremote_port: 80\nresponse_code: 200\n</code></pre></p> </li> </ol>","tags":["crc, helmchart, openshift"]},{"location":"blog/labs/helmchart-from-scratch/#updating-the-deployment","title":"Updating the deployment","text":"<p>To update a current deployment from helmchart it is used <code>helm upgrade</code>, using that command there are some different ways (documented here), for this example we will modify the <code>service.type</code> to use <code>NodePort</code> by modifying the <code>values.yaml</code></p> <ol> <li>After modified, list the current releases (must be ran inside the project context)</li> </ol> Note <p>Release is an instance of a chart running in a Kubernetes cluster</p> <pre><code>\u276f helm list\nNAME            NAMESPACE   REVISION    UPDATED                                     STATUS      CHART           APP VERSION\nmynginx-chart   default     1           2024-05-25 12:56:37.733458796 +0200 CEST    deployed    mynginx-0.1.0   1.16.0 \n</code></pre> <ol> <li> <p>Apply the modification. <pre><code>\u276f helm upgrade -f mynginx/values.yaml mynginx-chart mynginx/\nRelease \"mynginx-chart\" has been upgraded. Happy Helming!\nNAME: mynginx-chart\nLAST DEPLOYED: Sat May 25 13:57:55 2024\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 2\nNOTES:\n1. Get the application URL by running these commands:\n  export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].nodePort}\" services mynginx-chart-mynginx)\n  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n  echo http://$NODE_IP:$NODE_PORT\n</code></pre></p> </li> <li> <p>Inspect the release list and history of each release. <pre><code>\u276f helm list\nNAME            NAMESPACE   REVISION    UPDATED                                     STATUS      CHART           APP VERSION\nmynginx-chart   default     2           2024-05-25 13:57:55.817936502 +0200 CEST    deployed    mynginx-0.1.0   1.16.0 \n\n\u276f helm history mynginx-chart\nREVISION    UPDATED                     STATUS      CHART           APP VERSION DESCRIPTION     \n1           Sat May 25 12:56:37 2024    superseded  mynginx-0.1.0   1.16.0      Install complete\n2           Sat May 25 13:57:55 2024    deployed    mynginx-0.1.0   1.16.0      Upgrade complete\n</code></pre></p> </li> <li> <p>Validate that the pod is running, and because we are using nodePort using CRC/OpenShift local, due to limitations, we can only access via <code>port-forward</code>. Open a new terminal, or access via browser <code>127.0.0.1:32362</code>. <pre><code>\u276f oc port-forward svc/mynginx-chart-mynginx 32362:80\nForwarding from 127.0.0.1:32362 -&gt; 80\nForwarding from [::1]:32362 -&gt; 80\nHandling connection for 32362\n</code></pre></p> </li> </ol>","tags":["crc, helmchart, openshift"]},{"location":"blog/labs/images/","title":"Application using images from the internal registry","text":"<p>On this exercise<sup>1</sup>, we will deploy an application, tag and push a new version of the image to the Internal Registry, find where the image is hosted and patch the deployment to use the new image.</p>","tags":["podman, images, internal registry"]},{"location":"blog/labs/images/#steps","title":"Steps","text":"<ul> <li>Exposing the registry</li> <li>Application deployment</li> <li>Finding image inside the node</li> <li>Tagging and pushing an image to the internal registry</li> <li>Patching deployment image</li> </ul>","tags":["podman, images, internal registry"]},{"location":"blog/labs/images/#1-exposing-the-registry","title":"1. Exposing the registry","text":"<ul> <li> <p>Patch the Image Registry <pre><code> oc patch config.imageregistry.operator.openshift.io/cluster --patch='{\"spec\":{\"defaultRoute\":true}}' --type=merge\n oc patch config.imageregistry.operator.openshift.io/cluster --patch='[{\"op\": \"add\", \"path\": \"/spec/disableRedirect\", \"value\": true}]' --type=json\n</code></pre></p> </li> <li> <p>Take note of the registry route: <pre><code> oc get route -n openshift-image-registry default-route --template='{{ .spec.host }}'\n</code></pre></p> </li> </ul>","tags":["podman, images, internal registry"]},{"location":"blog/labs/images/#2-application-deployment","title":"2. Application deployment","text":"<ul> <li>This example will use an image that assumes authentication to Red Hat registry, but any other image can be used. <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: new-default-deploy\n    app.kubernetes.io/component: new-default-deploy\n    app.kubernetes.io/instance: new-default-deploy\n    app.kubernetes.io/part-of: new-default-deploy\n    app.openshift.io/runtime: redhat\n  name: new-default-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: new-default-deploy\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: new-default-deploy\n        deploymentconfig: new-default-deploy\n    spec:\n      containers:\n      - image: registry.access.redhat.com/ubi8/ubi:latest\n        imagePullPolicy: Always\n        name: new-default-deploy\n        command:\n        - /bin/sh\n        - -c\n        - |\n          sleep infinity\n        resources: {}\nEOF\n</code></pre></li> </ul>","tags":["podman, images, internal registry"]},{"location":"blog/labs/images/#3-finding-image-inside-the-node","title":"3. Finding image inside the node","text":"<ul> <li> <p>Inspect in which node the pod is hosted <pre><code>$ oc get pods -o wide\nNAME                                  READY   STATUS    RESTARTS   AGE    IP           NODE                                                NOMINATED NODE   READINESS GATES\nnew-default-deploy-786d477969-thcqt   1/1     Running   0          5m8s   10.128.2.6   hgomes-default-lab-grxh8-worker-westeurope3-xqdss   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> </li> <li> <p>Inspect the node where the image is hosted. <pre><code>$ oc debug node/hgomes-default-lab-grxh8-worker-westeurope3-xqdss\nsh-4.4# chroot /host\nsh-5.1# podman images | grep ubi\nregistry.access.redhat.com/ubi8/ubi             latest              179275e28757  3 days ago    213 MB\nsh-5.1# crictl images | grep ubi\nregistry.access.redhat.com/ubi8/ubi              latest               179275e28757e       213MB\n</code></pre></p> </li> </ul>","tags":["podman, images, internal registry"]},{"location":"blog/labs/images/#4-tagging-and-pushing-an-image-to-the-internal-registry","title":"4. Tagging and pushing an image to the internal registry","text":"<ul> <li> <p>Take note of your user token <pre><code>oc whoami -t\n</code></pre></p> </li> <li> <p>Tagging and pushing image to the internal registry.</p> <p>Use the exposed route to tag and push <pre><code>sh-5.1# podman login -u myuser -p &lt;token&gt;\nLogin Succeeded!\nsh-5.1# podman tag registry.access.redhat.com/ubi8/ubi:latest default-route-openshift-image-registry.apps.hhmkrp84.westeurope.aroapp.io/new-default-app/ubi8:latest\nsh-5.1# podman push default-route-openshift-image-registry.apps.hhmkrp84.westeurope.aroapp.io/new-default-app/ubi8:latest --remove-signatures\n</code></pre></p> </li> </ul>","tags":["podman, images, internal registry"]},{"location":"blog/labs/images/#5-patching-the-deployment-to-use-a-new-image","title":"5. Patching the deployment to use a new image","text":"<ul> <li> <p>Patch command to add the image recently pushed to the internal registry. <pre><code>oc patch deployment new-default-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"new-default-deploy\",\"image\":\"default-route-openshift-image-registry.apps.hhmkrp84.westeurope.aroapp.io/new-default-app/ubi8:latest\"}]}}}}\n</code></pre></p> </li> <li> <p>New pod running in the same node with the new image: <pre><code>oc get deployment -o wide\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS           IMAGES                                                                                                  SELECTOR\nnew-default-deploy   1/1     1            1           14m   new-default-deploy   default-route-openshift-image-registry.apps.hhmkrp84.westeurope.aroapp.io/new-default-app/ubi8:latest   app=new-default-deploy\n</code></pre></p> </li> </ul> <ol> <li> <p>This exercise was particularly interesting because it was how I was able to test and reproduce a bug once.\u00a0\u21a9</p> </li> </ol>","tags":["podman, images, internal registry"]},{"location":"blog/cheatsheet/jq-oneliner/","title":"JQ Oneliners","text":"<p>A set of oneliners that has been the most useful for me.</p>","tags":["JQ"]},{"location":"blog/cheatsheet/jq-oneliner/#jq","title":"JQ","text":"<p>jq is like sed for JSON data.</p>","tags":["JQ"]},{"location":"blog/cheatsheet/jq-oneliner/#listing-an-array-of-aws-instances-from-providerid-node-name-labels","title":"Listing an array of AWS instances from <code>providerID</code> + node name + labels","text":"OnelinerIdented <pre><code>for i in {i-001,i-002}; do jq -r --arg i \"$i\" '.items[] | select(.spec.providerID | contains($i)) | {node: $i, hostname: .metadata.labels.\"kubernetes.io/hostname\", roles: .metadata.labels | with_entries(select(.key | test(\"^node-role.kubernetes.io\")))}' my-nodes.json ; done\n</code></pre> <pre><code>for i in {i-001,i-002}; do\njq -r --arg i \"$i\" '\n    .items[] |\n    select(.spec.providerID | contains($i)) |\n    {\n        node: $i,\n        hostname: .metadata.labels.\"kubernetes.io/hostname\",\n        roles: .metadata.labels | with_entries(select(.key | test(\"^node-role.kubernetes.io\")))\n    }\n' my-nodes.json\ndone\n</code></pre> <p>Output: <pre><code>{\n  \"node\": \"i-001\",\n  \"hostname\": \"ip-10-001-002-34.eu-west-1.compute.internal\",\n  \"roles\": {\n    \"node-role.kubernetes.io\": \"infra\",\n    \"node-role.kubernetes.io/infra\": \"\",\n    \"node-role.kubernetes.io/worker\": \"\"\n  }\n</code></pre></p> <ul> <li> <p>Breakdown explanation</p> <ul> <li><code>test</code>: This is a jq function that tests if a string matches a regular expression pattern.</li> <li><code>\"^node-role.kubernetes.io\"</code>: This is the regular expression pattern that specifies the string to match. In this case, ^ matches the start of the string, and node-role.kubernetes.io is the pattern we want to match.</li> </ul> </li> </ul> <p>When applied to each key in the labels object, <code>test(\"^node-role.kubernetes.io\")</code> returns true if the key matches the pattern <code>\"^node-role.kubernetes.io\"</code> (i.e., if the key starts with \"node-role.kubernetes.io\"), and false otherwise.</p> <p>This filter function is used to select only the keys in the labels object that start with <code>node-role.kubernetes.io</code>, effectively filtering out other keys. This allows us to extract only the labels related to node roles from the labels object.</p>","tags":["JQ"]},{"location":"blog/cheatsheet/jq-oneliner/#get-specific-fields-values-from-multiple-containers-inside-a-pod","title":"Get specific fields values from multiple containers inside a pod","text":"OnelinerIdented <pre><code>oc get pods -o json | jq \".items[] | { pod_name: .metadata.name, containers: ( .spec.containers[].resources | { requests } ) }\"\n</code></pre> <pre><code>oc get pods -o json | jq '\n.items[] |\n{\n    pod_name: .metadata.name,\n    containers: (\n        .spec.containers[].resources |\n        {\n            requests\n            }\n        )\n    }\n'\n</code></pre> <p>Output: <pre><code>{\n  \"pod_name\": \"logging-loki-gateway-68d8b7744b-qvlw6\",\n  \"containers\": {\n    \"requests\": {\n      \"cpu\": \"500m\",\n      \"memory\": \"500Mi\"\n    }\n  }\n}\n{\n  \"pod_name\": \"logging-loki-gateway-68d8b7744b-qvlw6\",\n  \"containers\": {\n    \"requests\": null\n</code></pre></p>","tags":["JQ"]},{"location":"blog/cheatsheet/jq-oneliner/#extract-all-unique-usernames-from-a-json-file","title":"Extract all unique \"usernames\" from a json file","text":"<pre><code>cat data.json | jq .user.username -r | sort | uniq -c | sort -n\n</code></pre>","tags":["JQ"]},{"location":"blog/cheatsheet/jq-oneliner/#get-specific-fields-values-from-multiple-pods","title":"Get specific fields values from multiple pods","text":"<p>Get the pod name, the creationTimestamp and the node where the pod is hosted.</p> Oneliner <pre><code>  oc get pods -o json | jq -r '.items[] | .metadata.name + \" ===&gt; \" +.metadata.creationTimestamp + \" ===&gt; \" +.spec.nodeName' \n</code></pre> <p>Output: <pre><code>my-nginx-1 ===&gt; 2024-04-22T10:10:00Z ===&gt; worker-hevs-westeurope1-xxxx\nmy-nginx-2 ===&gt; 2024-04-23T10:10:00Z ===&gt; worker-hevs-westeurope1-xxxx\n</code></pre></p>","tags":["JQ"]},{"location":"blog/cheatsheet/mnemosyne-cheatsheet/","title":"Mnemosyne Cheatsheet","text":"<p>The greek god Mnemosyne hint me to write this collection of commands that from time to time slips my memory.</p>","tags":["cheatsheet, linux, podman"]},{"location":"blog/cheatsheet/mnemosyne-cheatsheet/#podman","title":"Podman","text":"Command Purpose podman exec $podname $command Running specific command to active container podman attach $podname Enter container","tags":["cheatsheet, linux, podman"]},{"location":"blog/cheatsheet/mnemosyne-cheatsheet/#system-commands","title":"System commands","text":"Fav Command Purpose journalctl --since \"30min ago\" Tracing logs in minutes find . -name filename.txt Looking for a file from current dir \u2b50 egrep \"string\" -rc * 2&gt;/dev/null | grep -v :0 Listing all dir and files that contains certain string \u2b50 stat -c%a $file Show the numerical permission values","tags":["cheatsheet, linux, podman"]},{"location":"blog/cheatsheet/mnemosyne-cheatsheet/#gnome","title":"Gnome","text":"Command Purpose Notes ALT + F8 Resize windows Shortcut ESC to cancel / Enter to Accept ALT + F7 Move windows Shortcut ESC to cancel / Enter to Accept","tags":["cheatsheet, linux, podman"]},{"location":"blog/labs/operator-sdk/","title":"Building the memcached-operator with podman and OCP","text":"<p>On this exercise we will go through the issues that I found while trying the example mem-cached operator.</p> <p>I used the steps from Operator-SDK page and hit issues caused by myself. \ud83d\ude42 This post is not a step by step from the Tutorial, but how I fixed the issues that I hit.</p>","tags":["operators,podman,ocp"]},{"location":"blog/labs/operator-sdk/#pre-requisites","title":"Pre-requisites","text":"<p>Original instructions for installations =&gt; here. But from my testing, at this time of writing were:</p> <ul> <li> <p>Binaries <pre><code>$ go version\ngo version go1.21.10 linux/amd64\n\n$ operator-sdk version\noperator-sdk version: \"v1.34.2\", commit: \"81dd3cb24b8744de03d312c1ba23bfc617044005\", kubernetes version: \"1.28.0\", go version: \"go1.21.10\", GOOS: \"linux\", GOARCH: \"amd64\"\n\n$ oc version\nClient Version: 4.13.18\n\n$ podman version\nClient:       Podman Engine\nVersion:      4.9.4\nAPI Version:  4.9.4\nGo Version:   go1.21.8\nBuilt:        Tue Mar 26 10:41:56 2024\nOS/Arch:      linux/amd64\n</code></pre></p> </li> <li> <p>Environment <pre><code>OpenShift 4.14.X\nFedora 38\n</code></pre></p> </li> </ul>","tags":["operators,podman,ocp"]},{"location":"blog/labs/operator-sdk/#issue-working-with-personalprivate-registries","title":"Issue: \"Working with personal/private registries\"","text":"<ul> <li>Do podman login to docker.io <pre><code>podman login -u $USER -p \"$PASSW\" docker.io\n</code></pre></li> <li>Update secret from the cluster with the docker.io credentials <pre><code>oc get secret/pull-secret -n openshift-config --template='{{index .data \".dockerconfigjson\" | base64decode}}' &gt; ocp-pullsecret.json\noc registry login --registry=docker.io --auth-basic=\"$USER:$PASSW\" --to=ocp-pullsecret.json\noc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=ocp-pullsecret.json\n</code></pre></li> </ul>","tags":["operators,podman,ocp"]},{"location":"blog/labs/operator-sdk/#issue-back-off-pulling-image-controllerlatest","title":"Issue: <code>Back-off pulling image \"controller:latest\"</code>","text":"<p>Hit back-off pulling image issues, because I mistakenly skipped the step to Setup the Operator Registry <pre><code>pod/memcached-operator-controller-manager-6d57548c9f-fdh67    Back-off pulling image \"controller:latest\"\nFailed to pull image \"controller:latest\": rpc error: code = Unknown desc = reading manifest latest in docker.io/library/controller: requested access to the resource is denied\n</code></pre> - Solution: In the file <code>Makefile</code>, set your own registry so the image can be built and pushed automatically to your registry, and then pulled inside the cluster. Eg: <pre><code>IMG = docker.io/songbird159/controller:latest\n</code></pre></p>","tags":["operators,podman,ocp"]},{"location":"blog/labs/operator-sdk/#issue-docker-command-not-found","title":"Issue: <code>docker: command not found</code>","text":"<p>Podman have all the equivalent functions as docker, simply use podman to run :) <pre><code>make podman-build podman-push\n</code></pre></p>","tags":["operators,podman,ocp"]},{"location":"blog/labs/operator-sdk/#issue-no-rule-to-make-target-podman-build","title":"Issue: <code>No rule to make target 'podman-build'</code>","text":"<p>This happens because in the <code>Makefile</code> it is still pointing to be used docker commands, not podman. <pre><code>make podman-build podman-push Makefile\nmake: *** No rule to make target 'podman-build'.  Stop.\n</code></pre> - Solution: Replace all docker references to podman. There are about 11 references. <pre><code>CONTAINER_TOOL ?= podman\n.PHONY: podman-build\npodman-build: ## Build docker image with the manager.\n\n.PHONY: podman-push\npodman-push: ## Push docker image with the manager.\n\n.PHONY: podman-buildx\npodman-buildx:\n\n.PHONY: bundle-build\nbundle-build: ## Build the bundle image.\n    podman build -f bundle.Dockerfile -t $(BUNDLE_IMG) .\n\n.PHONY: bundle-push\nbundle-push: ## Push the bundle image.\n    $(MAKE) podman-push IMG=$(BUNDLE_IMG)\n\ncatalog-build: opm ## Build a catalog image.\n    $(OPM) index add --container-tool podman --mode semver --tag $(CATALOG_IMG) --bundles $(BUNDLE_IMGS) $(FROM_INDEX_OPT)\n\n.PHONY: catalog-push\ncatalog-push: ## Push a catalog image.\n    $(MAKE) podman-push IMG=$(CATALOG_IMG)\n</code></pre></p>","tags":["operators,podman,ocp"]},{"location":"blog/labs/operator-sdk/#voila","title":"Voila!","text":"<pre><code>\u276f oc get events --sort-by='{.lastTimestamp}'\nSuccessfully pulled image \"docker.io/songbird159/controller:latest\" in 3.862300324s (3.86231446s including waiting)\n\n\u276f oc get pods -n memcached-operator-system\nNAME                                                     READY   STATUS    RESTARTS   AGE\nmemcached-operator-controller-manager-6d889bb7dd-j4d8q   2/2     Running   0          30m\n\n\u276f podman images\nREPOSITORY                                      TAG         IMAGE ID      CREATED         SIZE\ndocker.io/songbird159/controller                latest      4676737e0f28  51 minutes ago  55.4 MB\n</code></pre>","tags":["operators,podman,ocp"]},{"location":"blog/aro/public-ingresscontroller-aro-private/","title":"Setup public Ingress Controller to private ARO cluster","text":"<p>On this exercise, we will deploy an additional Ingress Controller to make application ingress available to the internet.</p>","tags":["aro, azure, ingresscontroller, openshift"]},{"location":"blog/aro/public-ingresscontroller-aro-private/#environment","title":"Environment","text":"<ul> <li> <p>Private ARO cluster (v4.12), with both <code>apiserver-visibility</code> and <code>ingress-visibility</code> set to Private. <pre><code>az aro create --resource-group $RESOURCEGROUP --name $CLUSTER --vnet aro-vnet --master-subnet master-subnet --worker-subnet worker-subnet --apiserver-visibility Private --ingress-visibility Private --pull-secret @pull-secret.txt\n</code></pre></p> </li> <li> <p>A \"jumphost\" VM  inside same cluster resource group.</p> </li> <li> <p>A DNS domain with hosting.</p> </li> </ul>","tags":["aro, azure, ingresscontroller, openshift"]},{"location":"blog/aro/public-ingresscontroller-aro-private/#if-ingress-is-public","title":"If Ingress is Public","text":"<p>If Ingress visibility is Public, then make sure to add the proper sharding to the default IngressController, otherwise, all requests will be routed to it. - About shards, here</p>","tags":["aro, azure, ingresscontroller, openshift"]},{"location":"blog/aro/public-ingresscontroller-aro-private/#setup","title":"Setup","text":"","tags":["aro, azure, ingresscontroller, openshift"]},{"location":"blog/aro/public-ingresscontroller-aro-private/#the-jumphost","title":"The jumphost","text":"<ol> <li>Create a virtual machine: <pre><code>az vm create --resource-group $RESOURCEGROUP --zone 1 --name 'hevs-jumphost' --image 'RedHat:RHEL:8-lvm-gen2:latest' --admin-username 'azureuser' --generate-ssh-keys --size 'Standard_D2s_v3'\n</code></pre></li> <li>Access the virtual machine via Azure Portal:</li> <li>Either using  the web feature SSH using Azure CLI which, quickly connect via the browser; Or any preferred method.</li> <li>OR habilitate the 22 Port via the UI, and using the Public IP provided, access using your private key. <pre><code>ssh -i ~/.ssh/id_rsa azureuser@&lt;publicIP&gt;\n</code></pre></li> </ol> Note <p>Make sure to assign permission 400 to the key. <code>chmod 400 ~/.ssh/id_rsa</code></p> <ol> <li> <p>Once inside the vm, download the <code>oc</code> client and add to path; oc client mirror, to download, here. <pre><code>$ wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/&lt;path&gt;.tar.gz\n$ tar -xf &lt;file&gt;.tar.gz\n$ sudo cp oc /usr/bin/oc\n</code></pre></p> </li> <li> <p>From the Azure Portal, in the <code>Azure Red Hat OpenShift</code> service, select the cluster, and in Connect to get the kubeadmin and password, as well the Cluster API. Then from the vm: <pre><code>$ oc login -u kubeadmin -p $PASSWORD --server $APIserverURL\n</code></pre></p> </li> </ol>","tags":["aro, azure, ingresscontroller, openshift"]},{"location":"blog/aro/public-ingresscontroller-aro-private/#creating-the-application","title":"Creating the application","text":"<ul> <li>For <code>hostname</code>, use the domain that you want to be used by the application. <pre><code>oc new-project demo-application\noc new-app --docker-image=docker.io/openshift/hello-openshift\noc expose svc hello-openshift --hostname example.hevshow.dns-dynamic.net\n</code></pre></li> </ul> Note <p>Got a free DNS domain and hosting in Cloudns.net : <code>hevshow.dns-dynamic.net</code></p>","tags":["aro, azure, ingresscontroller, openshift"]},{"location":"blog/aro/public-ingresscontroller-aro-private/#creating-the-ingress-controller","title":"Creating the Ingress Controller","text":"<p>Created a <code>Ingress Controller</code> named <code>public-ingress</code>, setting as the domain the same as what was my route, and added a <code>routerSelector</code> for my application, and the <code>loadBalancer.scope: External</code> <pre><code>spec:\n  domain: example.hevshow.dns-dynamic.net\n  routeSelector:\n    matchLabels:\n      app: hello-openshift\n  endpointPublishingStrategy:\n    loadBalancer:\n      scope: External\n    type: LoadBalancerService\n</code></pre></p> <ul> <li>Once the IC is created, pods and services in the project <code>openshift-ingress</code> will be created. <pre><code>oc get svc,pod -n openshift-ingress\n\nNAME                                  READY   STATUS    RESTARTS   AGE\npod/router-public-ingress-979d5c5bb-dpx59     1/1     Running   0          66m\npod/router-public-ingress-979d5c5bb-vzjfg     1/1     Running   0          66m\n\nNAME                                        TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                      AGE\nservice/router-public-ingress             LoadBalancer   172.30.35.87     108.141.255.176   80:32484/TCP,443:31682/TCP   66m\nservice/router-internal-public-ingress    ClusterIP      172.30.131.160   &lt;none&gt;            80/TCP,443/TCP,1936/TCP      66m\n</code></pre></li> </ul> Note <p>Take note of the <code>EXTERNAL-IP</code> for the SVC of Load Balancer type, and set as the IP for the A record in the DNS provider. <code>*.hevshow.dns-dynamic.net A 172.30.35.87</code></p>","tags":["aro, azure, ingresscontroller, openshift"]},{"location":"blog/aro/public-ingresscontroller-aro-private/#test-the-application","title":"Test the Application \ud83d\ude80","text":"<p>And voila, application is reachable for the internet! <pre><code>$ curl example.hevshow.dns-dynamic.net  \nHello OpenShift!\n</code></pre></p>","tags":["aro, azure, ingresscontroller, openshift"]},{"location":"blog/aro/public-ingresscontroller-aro-private/#extra-troubleshooting","title":"Extra - Troubleshooting \ud83d\udd75\ud83c\udffb","text":"<p>Check IP source <pre><code>nslookup example.hevshow.dns-dynamic.net\n</code></pre></p> <p>Resolves the domain using the Public IP from IC. (Use port 80 if 'http', 443 if 'https') <pre><code>curl --resolve example.hevshow.dns-dynamic.net:80:$IP http://example.hevshow.dns-dynamic.net --verbose\n</code></pre></p>","tags":["aro, azure, ingresscontroller, openshift"]},{"location":"blog/monitoring/promql%2Cmonitoring%2Cprometheus/","title":"PromQL expressions for K8S","text":"<p>A set of PROMQL expressions that has been the most useful for me.</p> <ul> <li> <p>What node got into Not Ready status? <pre><code>kube_node_status_condition{condition=\"Ready\",status!=\"true\"}\n</code></pre></p> </li> <li> <p>How many containers runs in a pod? The <code>~</code> works as a <code>*</code> wildcard/regex. <pre><code>kube_pod_container_info{cluster=\"\", namespace=\"&lt;project-name&gt;\",pod=~\"&lt;pod-name&gt;\"}\n</code></pre></p> </li> </ul>","tags":["promql","monitoring","prometheus"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/cheatsheet/","title":"Cheatsheet","text":""},{"location":"blog/category/aro/","title":"ARO","text":""},{"location":"blog/category/labs/","title":"Labs","text":""},{"location":"blog/category/monitoring/","title":"Monitoring","text":""},{"location":"blog/page/2/","title":"Index","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""}]}